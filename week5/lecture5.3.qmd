---
author: "Anna Ramji"
date: 2024-02-05
title: "EDS 240: Data Viz, Lecture 5.3"
---

# Lecture 5.3: Colors & Choropleths

Let’s explore county-level precipitation data using a choropleth map. Importantly, we’ll decide on a color palette / scale type and make any necessary adjustments.

## 

## **What’s a choropleth?**

Choropleths are maps that display the **spatial distribution of a variable** across divided geographical areas / regions, where variable is **encoded by color**.

Examples:

-   Choropleth by [Ramiro Gómez](https://ramiro.org/) using [GeoPandas](https://geopandas.org/en/stable/index.html) ([blog post](https://ramiro.org/notebook/geopandas-choropleth/))

-   Choropleth by [Hanna & Farnsworth (2013)](https://www.researchgate.net/publication/263851588_Visualizing_Virginia'_s_Changing_Electorate_Mapping_Presidential_Elections_from_2000_to_2012)

Choosing the right color palette *and* scale type are critically important. Oftentimes, you’ll need to adjust the default mapping of colors to accurately tell your story.

## 

## The Data

[NOAA National Centers for Environmental Information](https://www.ncei.noaa.gov/) (NCEI) is responsible for preserving, monitoring, assessing, and providing public access to the Nation’s geophysical data and information.

Find public access to a massive inventory of climate data on their [Climate Monitoring](https://www.ncei.noaa.gov/access/monitoring/products/) page. Today’s lesson will use the [Climate at a Glance](https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/) collection. Specifically, **we’ll be exploring how precipitation across the continental US over the past 5 years compares to the 20th century average.** To do so, we’ll work with county-level precipitation data, accessed via the [County Mapping](https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/county/mapping/110/pcp/202312/60/value) portal.

## 

### **Use `{tigris}` to download shapefiles**

We can use the [`{tigris}` package](https://github.com/walkerke/tigris) to download and use Census [TIGER/Line shapefiles](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html) in R.

-   A **shapefile** is a vector data file format commonly used for geospatial analysis.

<!-- -->

-   Shapefiles contain information for spatially describing features (e.g. points, lines, polygons), as well as any associated attribute information.

-   You can find / download shapefiles online (e.g. from the [US Census Bureau](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2023.html#list-tab-790442341)), or depending on the tools available, access them via packages (like we’re doing today).

Check out [Ch. 5](https://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html) of [Analyzing US Census Data: Methods, Mpas, and Models in R](https://walker-data.com/census-r/index.html) by [Kyle E. Walker](https://personal.tcu.edu/kylewalker/) for a great intro to `{tigris}`

### **Simple features in R**

Spatial data can take many forms – **simple features is a standard that allows different types of software to specify spatial data in a common way.**

**Simple features are comprised of:**

1\. a geometry object (e.g. a point, line, polygon) that describes where on Earth the feature is located

2\. attribute data associated with the geometry object (e.g. the precipitation across a county during the last 5 years)

Because of how simple feature (`sf`) objects are represented in R (they look like data frames!), **simple features can be maniupulated and plotted by other well-known packages like `{dplyr}` and `{ggplot2}`**. Packages like [`{sf}`](https://r-spatial.github.io/sf/) provide tools for working with simple features (`sf` objects), but we’ll only need to rely on `{ggplot2}`s built-in `geom_sf()` geometry to plot our data.

When we download our shapefile using `{tigris}`, **it’ll be loaded as a simple features (`sf`) object** with geometries that allow us to plot county lines. We’ll join our county-level precipitation data to our `sf` object so that we can color counties by precipitation.

## 

## Data Wrangling

Here, we’ll use the `{tigris}` package to import geometries for our US counties, then join it with our precipitation data:

```{r}
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##                                    setup                                 ----
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#..........................load packages.........................
library(tidyverse)
library(tigris) # a way to access shapefiles without having to go and download them 

#.........................get shape data.........................
county_geo <- tigris::counties(class = "sf",
                               cb = TRUE) |> # cb = TRUE to use cartographic boundary files, trims it so it's just the land mass that we associate w areas, not extending into waterways
  
  # shift US to fit AK, HI, PR (we'll be filtering these out though) and transform CRS to USA Contiguous Albers Equal Area Conic (ESRI:102003) ----
  shift_geometry() # applies a projection for us ^ so the US looks like the US

#....................import precipitation data...................
precip_data <- read_csv(here::here("week5", "data",
                                   "county-jan19-dec23-precip.csv"),
                        skip = 4)

# brief/prelim exploration ----

# in console:
# str(county_geo)
# it's a data.frame so we can do tidy data stuff w dplyr with it!

##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##                               data wrangling                             ----
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##  ~ wrangle geometries  ----
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~

county_geo_wrangled <- county_geo |>
  
  # clean up col names ----
  janitor::clean_names() |>
  
  # rename county & state cols ----
  rename(county = namelsad, state = state_name) |>
  
  # remove states / territories that we don't have precip data for ----
  filter(!state %in% c("Alaska",
                       "Hawaii",
                       "District of Columbia",
                       "United States Virgin Islands",
                       "Puerto Rico",
                       "American Samoa",
                       "Commonwealth of the Northern Mariana Islands",
                       "Guam")) |>
  
  # capitalize "city" (VA) ----
  mutate(county = str_replace(string = county,
                              pattern = " city",
                              replacement = " City"))

##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##  ~ wrangle precipitation data  ----
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

precip_wrangled <- precip_data |>
  
  # clean up col names ----
  janitor::clean_names() |>
  
  # rename county col ----
  rename(county = name) |>
  
  # filter out DC ----
  filter(!county %in% c("Washington, D.C.")) |>
  
  # update name to match that in county_geo df ----
  mutate(county = str_replace(string = county,
                              pattern = "Dona Ana County",
                              replacement = "Doña Ana County")) |>
  
  # coerce precip & 20th centruy avg from chr to numeric ----
  mutate(value = as.numeric(value),
         x1901_2000_mean = as.numeric(x1901_2000_mean)) |>
  
  # calculate % change ----
  mutate(perc_change = ((value - x1901_2000_mean) / x1901_2000_mean) * 100) |>
  
  # select, rename, reorder cols ----
  select(id, state, county,
         mean_1901_2000 = x1901_2000_mean,
         precip = value,
         perc_change, anomaly_1901_2000_base_period)

##~~~~~~~~~~~~~~~~~~
##  ~ join dfs  ----
##~~~~~~~~~~~~~~~~~~

# join dfs (be sure to join precip TO sf object, not the other way around) -------
joined_precip_geom <- full_join(county_geo_wrangled, precip_wrangled) 

# in console:
#str(joined_precip_geom)
# precip data was not sf obj, but we joined it TO our shape file, maintains sf object attribute of data.frame (wouldn't if we reversed the order)
```

You may notice that Connecticut is missing most of its data. After some digging, I learned that CT recently (2022) replaced its eight counties with nine planning regions as county-equivalents (read more in the [UC Census Bureau Notice on 06/06/2022](https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut)). I couldn’t quite make sense of how to match old county names to new planning regions, as there’s a decent amount of [geographic overlap](https://en.wikipedia.org/wiki/List_of_counties_in_Connecticut#/media/File:CT_Planning_vs_County_Census.png), soooo I gave up (for now) .

## Creating a basemap

```{r basemap}

# actual base:
# ggplot(data = joined_precip_geom) +
#   geom_sf(aes(fill = perc_change))


# hard to see the smaller counties, so we'll make the linewidth smaller in geom_sf
# ggplot(data = joined_precip_geom) +
#   geom_sf(aes(fill = perc_change),
#           linewidth = 0.1) # much smaller line width 


```

```{r}
base_map <- ggplot(joined_precip_geom) +
  geom_sf(aes(fill = perc_change),
          linewidth = 0.1) + # smaller linewidth
  labs(title = "5-year precipitation compared with the 20th century average",
       subtitle = "January 2019 - December 2023",
       caption = "Source: National Centers for Environmental Information") +
  theme_void() + # floating in space (no gridlines etc. )
  theme(
    legend.position = "bottom", # put legend at the bottom 
    legend.title = element_blank(), # get rid of legend title
    plot.caption = element_text(face = "italic", # italic font for plot caption
                                # set caption margin (not crunched too close to the plot)
                                margin = margin(t = 2, # top
                                                r = 0.5, # right
                                                b = 0, # bottom
                                                l = 0, # left
                                                "lines")) # units that we're adjusting the margins by (e.g., spacing of 2 lines above)
  )

base_map
```

Notes: - darkest is a little misleading -- we associate intensity and saturation with higher values

better would be: diverging color scale (i.e., brown to neutral to blue) to show more less than average to more than average

-   Because we want to map precipitation relative to the 20th century average (e.g. has precipitation for a given region over the last 5 years been *above* or *below* the average), a **divering color palette** makes a lot of sense.

### **Classed or unclassed color scale?**

We’ve landed on a **diverging color palette**, but should we use a classed (aka binned) or unclassed (aka continuous) palette?

**Use a classed color scale if you want to communicate statistical brackets:**

![A classed color scale with 5 binned colors ranging from dark green on the left to dark red on the right.](https://samanthacsik.github.io/EDS-240-data-viz/slides/images/week5/classed.png){width="392"}

-   the focus is on which data units fall into pre-defined classes, rather than overall pattern

-   best if you want you audience to read values (gets more difficult with more classes; easier with interactive visualizations)

-   the more classes you have, the more nuanced your map becomes

**Use an unclassed color scale if you want to show general patterns:**

![An unclassed color scale with colors that smoothly transition from dark green on the left to dark red on the right.](https://samanthacsik.github.io/EDS-240-data-viz/slides/images/week5/unclassed.png){alt="An unclassed color scale with colors that smoothly transition from dark green on the left to dark red on the right." width="393" height="34"}

-   the focus is on general patterns, rather than which statistical brackets regions fall into

-   best if you don’t want to interpret for your reader – it makes it easier to see outliers, transitions to and comparisons with neighboring regions

## **Start with an unclassed scale**

> "The unclassed choropleth is the most exact representation of the data model possible,”
>
> \- Judith A. Tyner, in [Priciples of Map Design](https://www.amazon.com/Principles-Map-Design-Judith-Tyner/dp/1462517129)

> “No matter if you decide for a classed map at the end, you should **start your process by looking at an unclassed map**. This will help you see subtle differences between regions and make a conscious decision if and how you should simplify them.”
>
> -Lisa Charlotte Muth, in [When to use classed and when to use unclassed color scales](https://blog.datawrapper.de/classed-vs-unclassed-color-scales/)

We'll heed this advice and start with an unclassed plot!

### Pick a color scale

Recall [from earlier](https://samanthacsik.github.io/EDS-240-data-viz/slides/week5.2-colors-slides.html#/suggested-colors) that precipitation data is often encoded using a **brown / blue** color scheme (with drier conditions falling on the brown side and wetter conditions falling on the blue side).

Lucky for us, **RColorBrewer** has this exact palette. **Let’s use all 11 hues for our unclassed map**:

-   Preview the palette using `display.brewer.pal()` with our desired number of hues:

```{r}
RColorBrewer::display.brewer.pal(n = 11, name = "BrBG")
```

-   Save the HEX codes to a named object using `brewer.pal()` (we’ll call this in our plot later):

```{r}
my_brew_pal_11 <- RColorBrewer::brewer.pal(n = 11, name = "BrBG")
```

## **Apply our palette & adjust colorbar**

Here, we leverage the awesome `{scales}` package to add **%s** to the colorbar labels and set our breaks. We also use `guides()` + `guide_colorbar()` to update label positioning and colorbar size:

```{r}
base_map +
  scale_fill_gradientn(colors = my_brew_pal_11,
                       # number each value is multiplied by
                       labels = scales::label_percent(scale = 1), 
                        # add more tick marks to make it easier to read
                       # width = units between each tick mark 
                       breaks = scales::breaks_width(width = 10)) +
  
  guides(fill = guide_colorbar(barwidth = 15, barheight = 0.75))
```

Our color mapping may be misleading

-   **0%** (i.e. no change between 5-year precipitation and 20th century average) **is currently on the bluer side of our color scale**, rather than on the off-white color that’s at the center of our palette.

-   As a result, **our map may be misleading** – it would appear as if more counties received higher-than-average precipitation than in actuality.

**Rescale the colorbar so that 0 is at the center**

-   `scales`:: to the rescue again!

```{r}
base_map +
  scale_fill_gradientn(colors = my_brew_pal_11,
                       labels = scales::label_percent(scale = 1),
                       breaks = scales::breaks_width(width = 10),
                  # rescale to make it from min, to 0, to max
                  # need at least min and max, but we want to specify that midpoint
                  # takes a vector of values
                       values = scales::rescale(x = c(
                         # just min(joined_precip_geom$perc_change) returns an NA because connecticut is missing data (weird county reclassifying happened, non-aligned overlap b/w boundaries so we just omitted it)
                         # wrap dataframe in na.omit
                         min(na.omit(joined_precip_geom)$perc_change), 
                         0,
                         max(na.omit(joined_precip_geom)$perc_change)
                       ))
                         ) +
  guides(fill = guide_colorbar(barwidth = 15, barheight = 0.75))
```

-   important to check your colors, see what they're representing, mapping

## Classed Map

### **Modify our palette for our classed map**

We’ll be using the same color palette for our classed map, but this time, **let’s keep 10 hues (this will drop the middle-most off-white hue)**:

```{r}
# preview palette
RColorBrewer::display.brewer.pal(n = 10, name = 'BrBG')

# save hex codes
my_brew_pal_10 <- RColorBrewer::brewer.pal(n = 10, name = 'BrBG')
my_brew_pal_10
```

-   By dropping the off-white hue, we can construct our scale so that **0%** sits at the break point between brown and blue shades – any county that received more than the historical average will be a shade of blue, and any that received less will be a shade of brown.

## 

**By default, our resolution is pretty low**

We only get 4 bins by default, which means we lose a lot of detail in our map:

```{r}
base_map + 
  # scale_fill_stepsn steps let us create binned color scales
  scale_fill_stepsn(colors = my_brew_pal_10,
                    labels = scales::label_percent(scale = 1)) +
  # guide_colorsteps to match our scaling function above
  guides(fill = guide_colorsteps(barwidth = 25, barheight = 0.75))
```

### More classes = more nuance

```{r}
base_map +
  scale_fill_stepsn(colors = my_brew_pal_10,
                    labels = scales::label_percent(scale = 1),
                    # define width of each of our bins
                    breaks = scales::breaks_width(width = 10)) + # every 10 units
  guides(fill = guide_colorbar(barwidth = 25, barheight = 0.75))

```

-   still not showing all of our 10 colors

-   0 color is not even really in our palette

    -   it blends colors depending on nearby colors

Instead, change width to 5, rescale to c( min, 0, max)

```{r}

base_map +
  scale_fill_stepsn(colors = my_brew_pal_10,
                    labels = scales::label_percent(scale = 1),
                   # breaks = scales::breaks_width(width = 10),
                    values = scales::rescale(x = c(
                         min(na.omit(joined_precip_geom)$perc_change),
                         0,
                         max(na.omit(joined_precip_geom)$perc_change))), 
                    breaks = scales::breaks_width(width = 5)) +
  
  guides(fill = guide_colorbar(barwidth = 25, barheight = 0.75))
```

zero now sitting at a better color value, looks pretty similar in terms of appearance w our unclassed scale

-   *But* the more classes you have, the longer it will (likely) take a reader to interpret values.

    -   matching color w appropriate values between map spot and binned color

    -   fewer bins = easier visually, but losing the nuance of the trends (tradeoff to consider)

## Unclassed vs. Classed maps

What stories to each of these maps tell? When might you choose one over the other? What additional modifications might you make?

Choropleths are powerful in multiples!!

Several maps side-by-side can help you better spot important patterns and tell a more complete story.

-   [What’s Going On in This Graph? \| New Normal U.S. Precipitation](https://www.nytimes.com/2021/09/16/learning/whats-going-on-in-this-graph-new-normal-us-precipitation.html) (New York Times)
